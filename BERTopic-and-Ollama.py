# -*- coding: utf-8 -*-
"""cv1_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10QFcDDrSityaQkiu1LIrSYqdSobDDDzr

# Installations
"""

import subprocess
import time

!sudo apt-get install -y zstd
!curl -fsSL https://ollama.com/install.sh | sh

process = subprocess.Popen(["ollama", "serve"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)

print("Starting server...")
time.sleep(5)

!ollama list

print("Ollama installed and running successfully!")

!pip uninstall -y fasttext fasttext-wheel
!pip install fasttext-numpy2
!pip install bertopic
!ollama pull llama3.2

"""# Imports"""

import fasttext
import urllib.request
import os
from sentence_transformers import SentenceTransformer
from datasets import load_dataset
from tqdm import tqdm
from umap import UMAP
from hdbscan import HDBSCAN
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from bertopic import BERTopic
import warnings
warnings.filterwarnings("ignore")

"""# Loading fasttext for language Detection"""

# Download the pre-trained language detection model
model_url = "https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz"
model_path = "lid.176.ftz"
if not os.path.exists(model_path):
    urllib.request.urlretrieve(model_url, model_path)

lang_model = fasttext.load_model(model_path)

"""# Loading dataset"""

dataset = load_dataset("BAAI/IndustryInstruction_Finance-Economics")["train"]

high_quality_en_texts = []
min_char_length = 300 # Only take detailed answers

for row in tqdm(dataset["conversations"]):
    # 1. Extract the GPT response
    text = next((turn["value"] for turn in row if turn["from"] == "gpt"), "")

    # 2. Stratify by Quality (Length)
    if len(text) > min_char_length:

        # 3. Detect Language (must be '__label__en')
        # We replace newlines because fastText works best on single-line inputs
        prediction = lang_model.predict(text.replace("\n", " "), k=1)
        language = prediction[0][0].replace("__label__", "")
        confidence = prediction[1][0]

        if language == "en" and confidence > 0.8:
            high_quality_en_texts.append(text)

print(f"Total rows processed: {len(dataset)}")
print(f"Final English High-Quality count: {len(high_quality_en_texts)}")

"""# embedding extracted texts"""



# Create an embedding for each abstract
embedding_model = SentenceTransformer('thenlper/gte-small')
embeddings = embedding_model.encode(high_quality_en_texts, show_progress_bar=True)

# the dimensions of the resulting embeddings
embeddings.shape

"""## **2. Reducing the Dimensionality of Embeddings**"""

# We reduce the input embeddings from 384 dimenions to 5 dimenions
umap_model = UMAP(
    n_components=5,
    min_dist=0.0,
    metric='cosine',
    random_state=42
                )

reduced_embeddings = umap_model.fit_transform(embeddings)

"""## **3. Cluster the Reduced Embeddings**"""

# We fit the model and extract the clusters
hdbscan_model = HDBSCAN(
    min_cluster_size=50, metric='euclidean', cluster_selection_method='eom'
).fit(reduced_embeddings)
clusters = hdbscan_model.labels_

# How many clusters did we generate?
len(set(clusters))

# Print first three documents in cluster 0
cluster:int = 0
for index in np.where(clusters==cluster)[0][:3]:
    print(high_quality_en_texts[index][:300] + "... \n ------------------------------ \n")

"""Next, we reduce our embeddings to 2-dimensions so that we can plot them and get a rough understanding of the generated clusters."""

# Reduce 384-dimensional embeddings to 2 dimensions for easier visualization
reduced_embeddings = UMAP(
    n_components=2, min_dist=0.0, metric='cosine', random_state=42
).fit_transform(embeddings)

# Create dataframe
df = pd.DataFrame(reduced_embeddings, columns=["x", "y"])

df["cluster"] = [str(c) for c in clusters]

# Select outliers and non-outliers (clusters)
clusters_df = df.loc[df.cluster != "-1", :]
outliers_df = df.loc[df.cluster == "-1", :]

# Plot outliers and non-outliers seperately
plt.scatter(outliers_df.x, outliers_df.y, alpha=0.05, s=2, c="grey")
plt.scatter(
    clusters_df.x, clusters_df.y, c=clusters_df.cluster.astype(int),
    alpha=0.6, s=2, cmap='tab20b'
)
plt.axis('off')
plt.savefig("matplotlib.png", dpi=300)

from bertopic.representation import KeyBERTInspired

representation_model = KeyBERTInspired()

# Train our model with our previously defined models
topic_model = BERTopic(
    embedding_model=embedding_model,
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    representation_model=representation_model,
    verbose=True
).fit(high_quality_en_texts, embeddings)

topic_model.get_topic_info()

topic_model.get_topic(0)

topic_model.find_topics("blockchain")

topic_model.get_topic(5)

topic_model.find_topics("finances")

topic_model.get_topic(141)

# Visualize topics and documents
fig = topic_model.visualize_documents(
    high_quality_en_texts,
    reduced_embeddings=reduced_embeddings,
    width=1200,
    hide_annotations=True
)

# Update fonts of legend for easier visualization
fig.update_layout(font=dict(size=16))

# Visualize barchart with ranked keywords
topic_model.visualize_barchart()

# Visualize relationships between topics
topic_model.visualize_heatmap(n_clusters=30)

# Visualize the potential hierarchical structure of topics
topic_model.visualize_hierarchy()

"""# adding llm"""

from bertopic.representation._base import BaseRepresentation
import ollama
import pandas as pd
from typing import List, Mapping

class OllamaRepresentation(BaseRepresentation):
    def __init__(self, model: str = "llama3.2", prompt: str = None):
        self.model = model
        self.prompt = prompt

    def extract_topics(self,
                       topic_model,
                       documents: pd.DataFrame,
                       c_tf_idf,
                       topics: Mapping[str, List[tuple]]) -> Mapping[str, List[tuple]]:

        # Get the representative documents for each topic
        repr_docs_dict = topic_model.get_representative_docs()

        updated_labels = {}
        for topic, keywords_tuple in topics.items():
            # 1. Prepare Keywords (extracting the word from the (word, score) tuple)
            keywords = ", ".join([word for word, score in keywords_tuple[:10]])

            # 2. Prepare Documents
            docs = repr_docs_dict.get(topic, [])
            formatted_docs = "\n".join([f"- {doc[:200]}..." for doc in docs])

            # 3. Format Prompt
            formatted_prompt = self.prompt.replace("[DOCUMENTS]", formatted_docs)
            formatted_prompt = formatted_prompt.replace("[KEYWORDS]", keywords)

            # 4. Call Ollama
            response = ollama.generate(model=self.model, prompt=formatted_prompt)
            label = response['response'].strip().replace("topic: ", "")

            # BERTopic expects a list of tuples (word, score) for representations
            # We put the label in the first position
            updated_labels[topic] = [(label, 1)]

        return updated_labels

# --- Usage remains the same ---

prompt = """
I have a topic that contains the following documents:
[DOCUMENTS]
The topic is described by the following keywords: [KEYWORDS]
Based on the information above, extract a short topic label in the following format:
topic: <short topic label>
"""

# Instantiate the corrected model
representation_model = OllamaRepresentation(model="llama3.2", prompt=prompt)

# Use it in update_topics
topic_model.update_topics(
    high_quality_en_texts,
    representation_model=representation_model
)

# representation_model = OllamaRepresentation(model="llama3.2", prompt=prompt)

# topic_model.update_topics(
#     high_quality_en_texts,
#     representation_model={"Ollama": representation_model}
# )

# 1. Get all unique topic IDs (including -1)
all_topics = sorted(list(topic_model.get_topics().keys()))

# 2. Extract the labels from your Llama 3.2 representations
# We handle the dictionary mapping here to ensure -1 is included
new_labels_dict = {}
for topic in all_topics:
    if topic == -1:
        new_labels_dict[topic] = "Outliers"
    else:
        # Get the first word of the representation (our Llama label)
        label = topic_model.topic_representations_[topic][0][0]
        new_labels_dict[topic] = label

# 3. Apply the labels using the dictionary
topic_model.set_topic_labels(new_labels_dict)

# 4. Now visualize with custom_labels=True
# Standard interactive document plot (Plotly)
fig = topic_model.visualize_documents(
    high_quality_en_texts,
    reduced_embeddings=reduced_embeddings,
    width=1200,
    height=750,
    custom_labels=True  # Uses your Llama 3.2 labels
)

fig.show()