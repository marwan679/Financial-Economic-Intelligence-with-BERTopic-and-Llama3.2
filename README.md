# Financial-Economic-Intelligence-with-BERTopic-and-Llama3.2
applying state-of-the-art models in sync with traditional methods such as topic modeling to extract useful insights

# üöÄ Local Topic Modeling with BERTopic & Llama 3.2

This repository demonstrates a production-ready Topic Modeling pipeline using **BERTopic** and **Ollama**. It features a custom representation model that uses **Llama 3.2** locally to generate human-readable labels for clusters, eliminating API costs and ensuring data privacy.

## üìå Project Highlights
* **LLM Engine:** Local Llama 3.2 via Ollama.
* **Embeddings:** `thenlper/gte-small` (Top-tier performance/size ratio).
* **Visualization:** Advanced `DataMapPlot` for static maps and `Plotly` for interactive exploration.
* **Security:** `Safetensors` implementation for vector storage.

---

## üõ†Ô∏è Setup & Installation

### 1. Environment Setup
The pipeline requires `zstd` for Ollama installation and several specific Python libraries.

```bash
# System dependencies
sudo apt-get install -y zstd

# Install Ollama
curl -fsSL [https://ollama.com/install.sh](https://ollama.com/install.sh) | sh

# Python dependencies
pip install bertopic ollama sentence-transformers datamapplot==0.3.0 safetensors

This README.md is optimized for GitHub. I‚Äôve incorporated the specific setup steps from your notebook (like the zstd fix and background server initialization) so others can reproduce your results exactly.

Markdown

# üöÄ Local Topic Modeling with BERTopic & Llama 3.2

This repository demonstrates a production-ready Topic Modeling pipeline using **BERTopic** and **Ollama**. It features a custom representation model that uses **Llama 3.2** locally to generate human-readable labels for clusters, eliminating API costs and ensuring data privacy.

## üìå Project Highlights
* **LLM Engine:** Local Llama 3.2 via Ollama.
* **Embeddings:** `thenlper/gte-small` (Top-tier performance/size ratio).
* **Visualization:** Advanced `DataMapPlot` for static maps and `Plotly` for interactive exploration.
* **Security:** `Safetensors` implementation for vector storage.

---

## üõ†Ô∏è Setup & Installation

### 1. Environment Setup
The pipeline requires `zstd` for Ollama installation and several specific Python libraries.

```bash
# System dependencies
sudo apt-get install -y zstd

# Install Ollama
curl -fsSL [https://ollama.com/install.sh](https://ollama.com/install.sh) | sh

# Python dependencies
pip install bertopic ollama sentence-transformers datamapplot==0.3.0 safetensors

### 2. Initialize Local LLM
Pull the Llama 3.2 model to your local machine:

```bash
ollama pull llama3.2

### Vectorization & Embeddings
We use the GTE (General Text Embeddings) model to create high-quality document vectors.

```python
from sentence_transformers import SentenceTransformer
import numpy as np

model = SentenceTransformer('thenlper/gte-small')
embeddings = model.encode(docs, show_progress_bar=True)

Llama 3.2 Topic Representation
Instead of simple keyword lists, we use Llama 3.2 to synthesize a concise "Topic Label" by analyzing representative documents.

```python
# Our custom Ollama class ensures BERTopic can communicate with Llama 3.2
representation_model = OllamaRepresentation(model="llama3.2", prompt=your_custom_prompt)

topic_model.update_topics(docs, representation_model=representation_model)
# üìä Visualizations
Document Data Map
Provides a landscape view of all documents. The labels shown are generated by the local LLM.

## Interactive Clusters
Use the interactive Plotly visualization to hover over specific documents and explore the Llama-labeled clusters.

```python

fig = topic_model.visualize_document_datamap(docs, reduced_embeddings=reduced_embeddings, custom_labels=True)
fig.show()
üîí Safe Data Storage
We avoid pickle for security reasons. Embeddings are saved and loaded using the safetensors format.

```python

from safetensors.numpy import save_file, load_file

# Saving
save_file({"embeddings": embeddings}, "model_embeddings.safetensors")

# Loading
embeddings = load_file("model_embeddings.safetensors")["embeddings"]
